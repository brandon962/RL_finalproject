{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10857413352852538209\n",
      "]\n",
      "tensorflow version:  1.10.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print('tensorflow version: ',tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'bird' # the name of the game being played for log files\n",
    "ACTIONS = 2 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVE = 1000. # timesteps to observe before training\n",
    "EXPLORE = 2000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.9 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork():\n",
    "    # network weights\n",
    "    W_conv1 = weight_variable([8, 8, 4, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    W_conv2 = weight_variable([4, 4, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "\n",
    "    W_fc1 = weight_variable([1600, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "\n",
    "    W_fc2 = weight_variable([512, ACTIONS])\n",
    "    b_fc2 = bias_variable([ACTIONS])\n",
    "\n",
    "    # input layer\n",
    "    s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
    "\n",
    "    # hidden layers\n",
    "    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
    "    #h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
    "    #h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "    #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
    "    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
    "\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # readout layer\n",
    "    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "    return s, readout, h_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(s, readout, h_fc1, sess):\n",
    "    # define the cost function\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "    # open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "\n",
    "    # printing\n",
    "    a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
    "    h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
    "\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    # saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    # start training\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    \n",
    "    while \"flappy bird\" != \"angry bird\":\n",
    "        # choose an action epsilon greedily\n",
    "        readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            if random.random() <= epsilon:\n",
    "                print(\"----------Random Action----------\")\n",
    "                \n",
    "                if random.random() < 0.1 :\n",
    "                    a_t[1] = 1\n",
    "                    action_index = 1\n",
    "                else:\n",
    "                    a_t[0] = 1\n",
    "                    action_index = 0\n",
    "            else:\n",
    "                action_index = np.argmax(readout_t)\n",
    "                a_t[action_index] = 1\n",
    "        else:\n",
    "            a_t[0] = 1 # do nothing\n",
    "\n",
    "        # scale down epsilon\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observe next state and reward\n",
    "        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "        x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "\n",
    "        # store the transition in D\n",
    "        D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        # only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "\n",
    "            # get the batch variables\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "            y_batch = []\n",
    "            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "            for i in range(0, len(minibatch)):\n",
    "                terminal = minibatch[i][4]\n",
    "                # if terminal, only equals reward\n",
    "                if terminal:\n",
    "                    y_batch.append(r_batch[i])\n",
    "                else:\n",
    "                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "            # perform gradient step\n",
    "            train_step.run(feed_dict = {\n",
    "                y : y_batch,\n",
    "                a : a_batch,\n",
    "                s : s_j_batch}\n",
    "            )\n",
    "\n",
    "        # update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        if t % 10000 == 0:\n",
    "            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
    "            \"/ Q_MAX %e\" % np.max(readout_t))\n",
    "        # write info to files\n",
    "        '''\n",
    "        if t % 10000 <= 100:\n",
    "            a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
    "            h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
    "            cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\flash\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Could not find old network weights\n",
      "----------Random Action----------\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.810054e-02\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.745290e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.768072e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.832614e-02\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.782294e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.938657e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.676813e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 8 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.655977e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 9 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.796257e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 10 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.360558e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 11 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.667025e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 12 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.818748e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 13 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.470615e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 14 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.148049e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 15 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 3.189479e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 16 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.237034e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 17 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.998463e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 18 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.636931e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 19 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.349930e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 20 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.303588e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 21 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.055785e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 22 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.014666e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 23 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.669171e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 24 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.702712e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 25 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.059728e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 26 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.104105e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 27 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.406388e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 28 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.056071e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 29 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.057050e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 30 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 1.908780e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 31 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 1.866774e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 32 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.025205e-02\n",
      "TIMESTEP 33 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.042159e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 34 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 1.969137e-02\n",
      "TIMESTEP 35 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.407832e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 36 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 1.969548e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 37 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 1.894321e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 38 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 1.808281e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 39 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.295764e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 40 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.109890e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 41 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.512241e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 42 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.548345e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 43 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.693953e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 44 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.140371e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 45 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.432217e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 46 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.017394e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 47 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.360724e-02\n",
      "TIMESTEP 48 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.360892e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 49 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD -1 / Q_MAX 2.489797e-02\n",
      "TIMESTEP 50 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.168602e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 51 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.782523e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 52 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.575949e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 53 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.593568e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 54 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.528957e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 55 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.616501e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 56 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.864648e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 57 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.054528e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 58 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.048280e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 59 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.201729e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 60 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.431203e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 61 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.456080e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 62 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.494659e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 63 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.440639e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 64 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.236628e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 65 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.403094e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 66 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.550900e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 67 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.128295e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 68 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.137235e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Random Action----------\n",
      "TIMESTEP 69 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.959162e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 70 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.661210e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 71 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.427261e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 72 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.507914e-02\n",
      "TIMESTEP 73 / STATE observe / EPSILON 0.9 / ACTION 1 / REWARD 0.1 / Q_MAX 2.901403e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 74 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 3.102513e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 75 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.134015e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 76 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.472691e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 77 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.424090e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 78 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 1.708188e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 79 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 1.917545e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 80 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.311603e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 81 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 1.583616e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 82 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.266462e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 83 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 1.917968e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 84 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.340920e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP 85 / STATE observe / EPSILON 0.9 / ACTION 0 / REWARD 0.1 / Q_MAX 2.039030e-02\n",
      "----------Random Action----------\n",
      "TIMESTEP"
     ]
    }
   ],
   "source": [
    "def playGame():\n",
    "    sess = tf.InteractiveSession()\n",
    "    s, readout, h_fc1 = createNetwork()\n",
    "    trainNetwork(s, readout, h_fc1, sess)\n",
    "\n",
    "def main():\n",
    "    playGame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
